# Development Approach for the Advent of Code Automation Project

## Purpose

The goal of this project is to build an end-to-end solution for solving Advent of Code challenges using **AI-assisted development**, primarily with **Aider Chat**. The project will follow an iterative, test-driven approach, ensuring high-quality, maintainable, and open-source-friendly code.

---

## Development Workflow

1. **Task Breakdown and Planning**  
   - Define clear, incremental tasks with specific goals and test criteria.  
   - Document tasks in a structured format for tracking progress.  

2. **AI-Assisted Development**  
   - Use **Aider Chat** to assist with:  
     - Code generation.  
     - Code improvements and refactoring.  
     - Explanations and optimizations.  
   - Utilize **DeepSeek R1** for architecture and planning.  
   - Use **DeepSeek V3** for implementation and debugging.

3. **Incremental Development Steps**  
   - Define the task.  
   - Generate an initial implementation with AI.  
   - Review and refine the AI-generated code.  
   - Implement and run tests.  
   - Accept or request further modifications.  

4. **Testing and Validation**  
   - Ensure functional correctness with automated tests.  
   - Validate edge cases and expected behaviors.  
   - Use Python testing frameworks for verification.

5. **Code Review and Documentation**  
   - Regularly review AI-generated code for improvements.  
   - Maintain clear and concise documentation for contributors.  

6. **Version Control and CI/CD**  
   - Use GitHub for source control and collaboration.  
   - Set up automated tests to ensure code quality.  

---

## Tools and Technologies

- **AI Tools:** Aider Chat (DeepSeek R1, DeepSeek V3)  
- **Programming:** Python, Requests, BeautifulSoup  
- **Version Control:** GitHub  
- **Testing:** Pytest  
- **Documentation:** Markdown  

---

## Best Practices

- Develop incrementally with clear milestones.  
- Use test-driven development to validate progress.  
- Review AI-generated code critically before acceptance.  
- Keep the project structure clean and maintainable.  
- Engage with the open-source community for feedback.  

---

## Next Steps

1. Set up the GitHub repository.  
2. Define initial tasks for project setup and authentication.  
3. Begin AI-assisted implementation and testing.  


---

## First phase generated tasks
These tasks were generated by OpenAI O1 model with prompting it to follow my output rules and trying to focus on the making 
smaller tasks which is about a junior developer or medior developer level. I don't really modify anything in this and these 
tasks will be my main step list to implement the solution with Aider.chat and with the selected models. 

Each step will follow the following general development flow:
1. feed the task to with aider
2. check the diff it is created
3. i hope i will normally accept it but maybe give it suggestions to create
4. test the output of the code and if further improvement is needed i will add intermediate TASK to Aider to implement that extension

### AIDER config
I added a CONVENTIONS.md file which is configured globally for myself as a general READ input for Aider.

Than I added an .aiderignore file to the project to skip aider using every unnecessary files in the project.

I have upgraded Aider to v.0.72.3 as an actual version in which R1 models are supperted already.

To start AIDER with DeepSeek R1 as architect and DeepSeek V3 as editor model I start aider with
```
aider --architect --model r1 --editor-model deepseek
```

possible improvements:
- aider can automatically always call tests (lint, test run, etc.) so basic problems can be corrected immediately
- i hope the whole solution won't be big but if needed i have to make Aider to make the code to smaller parts and files
- i hope the whole solution won't be big but if it will be i have to think about possible improvements how to add more detail and context to the TASKs
- this whole task list is a good approach to solve the problem but it is pure AI output (my original thougths are included but almost one-shot output of OpenAI O1 model). So I am sure I have to add more intermediate or post TASKs to solve my original goal.


From now on these are the tasks I get as a starting output:

```
<TASKS>

<TASK>
   <TASK_PURPOSE>Set up the core project structure and initialize version control</TASK_PURPOSE>
   <TASK_INSTRUCTION>Create the main project folder (e.g., "aoc_project")</TASK_INSTRUCTION>
   <TASK_INSTRUCTION>Initialize Git by running "git init" inside the folder</TASK_INSTRUCTION>
   <TASK_INSTRUCTION>Create a .gitignore file to exclude environment files and build artifacts</TASK_INSTRUCTION>
   <TASK_TEST>Check if "aoc_project" folder is created</TASK_TEST>
   <TASK_TEST>Check if ".gitignore" file is created</TASK_TEST>
   <TASK_TEST>Verify ".git" directory exists, confirming Git initialization</TASK_TEST>
</TASK>

<TASK>
   <TASK_PURPOSE>Create a basic README and project environment setup instructions</TASK_PURPOSE>
   <TASK_INSTRUCTION>Create a "README.md" in the project root with a brief description of the project goals</TASK_INSTRUCTION>
   <TASK_INSTRUCTION>Include a section explaining how to set up the environment (e.g. Python version, virtualenv usage)<TASK_INSTRUCTION>
   <TASK_INSTRUCTION>Optionally add a section for future contributors on how to run or test the application</TASK_INSTRUCTION>
   <TASK_TEST>Check if "README.md" is created with minimal project details</TASK_TEST>
   <TASK_TEST>Verify environment setup instructions are present in "README.md"</TASK_TEST>
</TASK>

<TASK>
   <TASK_PURPOSE>Implement basic configuration file and loading logic</TASK_PURPOSE>
   <TASK_INSTRUCTION>Create a config file named "config.json" with placeholders: sessionCookie, baseDirectory</TASK_INSTRUCTION>
   <TASK_INSTRUCTION>Create a small module or script (e.g., "config_loader.py") to read and parse "config.json"</TASK_INSTRUCTION>
   <TASK_INSTRUCTION>Set default baseDirectory to "tasks" if not provided in config</TASK_INSTRUCTION>
   <TASK_INSTRUCTION>Handle validation (e.g., ensure sessionCookie is not empty)</TASK_INSTRUCTION>
   <TASK_TEST>Check if "config.json" exists and contains the required keys</TASK_TEST>
   <TASK_TEST>Run "config_loader.py" to confirm it correctly prints or returns sessionCookie and baseDirectory</TASK_TEST>
   <TASK_TEST>Ensure baseDirectory defaults to "tasks" when not specified</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Implement session cookie authentication check with Advent of Code</TASK_PURPOSE>
    <TASK_INSTRUCTION>In a module or function (e.g., "auth_checker.py"), read sessionCookie from config</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Make an HTTP GET request to "https://adventofcode.com" using the sessionCookie</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Check if the response indicates a successful login (e.g., status code 200 and not a "please log in" message)</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>If authentication fails, print an error and exit</TASK_INSTRUCTION>
    <TASK_TEST>Test with a valid sessionCookie to confirm it passes authentication</TASK_TEST>
    <TASK_TEST>Test with an invalid sessionCookie to ensure the script reports an error</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Implement puzzle discovery logic (CLI argument parsing and AI-based approach)</TASK_PURPOSE>
    <TASK_INSTRUCTION>Create a new module or script (e.g., "puzzle_discovery.py")</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Parse command-line arguments for "--day" (if provided, use that day by default)</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>If no day is provided, fetch the AoC homepage HTML and parse it (optionally using AI or a regex-based approach) to find the first unsolved day/part</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Store the discovered day and part in variables or return them from the function</TASK_INSTRUCTION>
    <TASK_TEST>Run the script with "--day=5" to confirm day=5 is correctly discovered</TASK_TEST>
    <TASK_TEST>Run the script without "--day" and mock a homepage response that indicates the first unsolved puzzle is day=3 part=2, confirm the logic returns (3,2)</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Prevent overwriting an existing day/part directory</TASK_PURPOSE>
    <TASK_INSTRUCTION>After discovering the day and part, build a folder name like "DAY_PART" (e.g., "6_1")</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Check if the folder already exists in baseDirectory</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>If it does exist, exit with a message instead of overwriting</TASK_INSTRUCTION>
    <TASK_TEST>Confirm that if "tasks/6_1" already exists, the script stops</TASK_TEST>
    <TASK_TEST>Confirm that if the directory doesn’t exist, the script continues</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Create the puzzle directory and fetch puzzle HTML</TASK_PURPOSE>
    <TASK_INSTRUCTION>Make a directory "<baseDirectory>/<day>_<part>" (e.g., "tasks/6_1")</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Perform a GET request to "https://adventofcode.com/2024/day/<day>" to get puzzle content</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Save the raw HTML to a file (e.g., "puzzle_raw.html") in the newly created directory</TASK_INSTRUCTION>
    <TASK_TEST>Check that "tasks/6_1" (or the discovered day/part) directory is created</TASK_TEST>
    <TASK_TEST>Verify that "puzzle_raw.html" is saved and is not empty</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Fetch the puzzle input and store it for further processing</TASK_PURPOSE>
    <TASK_INSTRUCTION>Create a subdirectory "input" inside "<baseDirectory>/<day>_<part>"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Perform a GET request to "https://adventofcode.com/2024/day/<day>/input"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Save the content to "input/my_input.txt"</TASK_INSTRUCTION>
    <TASK_TEST>Check if "input" folder exists inside the day/part directory</TASK_TEST>
    <TASK_TEST>Confirm "my_input.txt" is successfully created and not empty</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Convert puzzle HTML into a markdown file for easier reading</TASK_PURPOSE>
    <TASK_INSTRUCTION>Create a function (e.g., "convert_html_to_markdown") that reads "puzzle_raw.html"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Convert the HTML structure (paragraphs, headings) into basic markdown syntax</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Save the resulting text into "task.md" in the same directory</TASK_INSTRUCTION>
    <TASK_TEST>Verify "task.md" is created</TASK_TEST>
    <TASK_TEST>Check that at least some HTML elements (e.g., paragraphs, headers) have been converted to markdown</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Extract code blocks and example inputs/outputs from the puzzle content</TASK_PURPOSE>
    <TASK_INSTRUCTION>Within the "convert_html_to_markdown" or a separate function, detect "<code>" blocks in the HTML</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Convert each "<code>" block into a fenced code block (e.g., triple backticks) in the markdown</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Look for puzzle examples in the text (e.g., "For example" sections) and separate them into "example-in-<N>.txt" and "example-out-<N>.txt"</TASK_INSTRUCTION>
    <TASK_TEST>Check that code blocks are properly wrapped with triple backticks in "task.md"</TASK_TEST>
    <TASK_TEST>If examples exist, confirm that corresponding "example-in-<N>.txt" and "example-out-<N>.txt" files are created</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Create a simple placeholder solver script</TASK_PURPOSE>
    <TASK_INSTRUCTION>In "<baseDirectory>/<day>_<part>", create a file named "solve.py"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Implement a minimal main function that reads "my_input.txt" (if it exists) and prints a placeholder result like "0"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Include instructions/comments in "solve.py" that it will be extended later</TASK_INSTRUCTION>
    <TASK_TEST>Check that "solve.py" is created</TASK_TEST>
    <TASK_TEST>Run "solve.py" and verify it prints the placeholder output</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Implement testing with any extracted examples</TASK_PURPOSE>
    <TASK_INSTRUCTION>Create or extend a test script (e.g., "test_examples.py") that finds all "example-in-<N>.txt"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Run "solve.py" against each example input file and capture the output</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Compare the solver’s output with the content of "example-out-<N>.txt"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Fail or log an error if mismatches occur</TASK_INSTRUCTION>
    <TASK_TEST>Mock a simple example to check if the test script can detect success or failure</TASK_TEST>
    <TASK_TEST>Run "test_examples.py" to confirm the test process works as expected</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Execute the solver with the real input and prepare for submission</TASK_PURPOSE>
    <TASK_INSTRUCTION>Update "solve.py" (or a separate script) to run the solution logic on "my_input.txt"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Write the solver’s output to a file like "result_part1.txt" or print it to stdout</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Ensure any logs or errors are handled gracefully</TASK_INSTRUCTION>
    <TASK_TEST>Check if "result_part1.txt" is created after running the solver</TASK_TEST>
    <TASK_TEST>Confirm the file is not empty</TASK_TEST>
</TASK>

<TASK>
    <TASK_PURPOSE>Implement submission of the puzzle answer to Advent of Code</TASK_PURPOSE>
    <TASK_INSTRUCTION>Create a function (e.g., "submit_answer") that POSTs the puzzle answer and part number to "https://adventofcode.com/2024/day/<day>/answer"</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Check the response for success or failure messages and log the outcome</TASK_INSTRUCTION>
    <TASK_INSTRUCTION>Handle any response indicating the puzzle part is locked or the answer is incorrect</TASK_INSTRUCTION>
    <TASK_TEST>Mock a successful submission response and confirm the script logs success</TASK_TEST>
    <TASK_TEST>Mock an incorrect answer response and confirm the script logs failure</TASK_TEST>
</TASK>

</TASKS>

```


I won't say i write about every step i create cause it requires a lot of time from me to document everything. So I will write only the most important parts here.
After I have started the Aider with the command i provided above i get into the architect mode and i could see that Ader already use my CONVENTIONS.md.

Now I will start adding the new tasks to it...

## TASK 1
was success but have to solve some issues
- have to modify the toml file to be sync with my account (OK)
- i updated the toml python and uv version (python 3.12 and uv 0.5.24) (OK)
- i updated my UV (was not necessary but i felt like i should) (OK)
- i have to deleted the lockfile part from the toml file cause it cannot be parsed (WARN)

ERRORS:
- it said i have to do this but it doesn't created the venv for me so i have to created it myself with uv
1. Activate the virtual environment: `source .venv/bin/activate`
2. Install any dependencies: `uv pip install -r requirements.in`
3. Make initial commit: `git add . && git commit -m "Initial project setup"`
- maybe init the project with uv directly and add the modifications after that would be better (uv init + than use the outputs)
- after i init the project myself i could run the commands above and now i am sync and finished with the first TASK

## TASK 2
was success but cause i already have a README.md file at some part of the requirements were already there it case small problems to it.

ERRORS:
- it used classic python and not UV (I think the main problem is that uv is still quite new and most of the training codes use the classical solutions and that is soo strong that the model easily change back to it)
